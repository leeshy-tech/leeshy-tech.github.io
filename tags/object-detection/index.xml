<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>object detection - 标签 - Leeshy&#39;s Blog | To be humble</title>
        <link>https://leeshy-tech.github.io/tags/object-detection/</link>
        <description>object detection - 标签 - Leeshy&#39;s Blog | To be humble</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>saili@bupt.edu.cn (Leeshy)</managingEditor>
            <webMaster>saili@bupt.edu.cn (Leeshy)</webMaster><lastBuildDate>Sat, 18 Feb 2023 15:11:06 &#43;0800</lastBuildDate><atom:link href="https://leeshy-tech.github.io/tags/object-detection/" rel="self" type="application/rss+xml" /><item>
    <title>YOLO v3深度解读</title>
    <link>https://leeshy-tech.github.io/yolo_v3/</link>
    <pubDate>Sat, 18 Feb 2023 15:11:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/yolo_v3/</guid>
    <description><![CDATA[<h1 id="yolov3-an-incremental-improvement">YOLOv3: An Incremental Improvement</h1>
<h2 id="概况">概况</h2>
<p><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener noreffer">YOLOv3: An Incremental Improvement</a></p>
<p>arxiv 2018</p>
<h2 id="算法">算法</h2>
<h3 id="骨干网络">骨干网络</h3>
<p>模型层数变深，并加入了残差连接。</p>
<p></p>
<p>Darknet-53实现了精度和速度的折中。</p>
<p></p>
<p>输入416*416的图像，骨干网络是一个全卷积网络，去除了全连接层，输出3个尺度的特征图，对应不同数量的grid cell（13 26 52），不同感受野负责不同尺度物体的预测。特征金字塔（Feature Pyramid Networks）思想。</p>
<p></p>
<p></p>
<h3 id="损失函数">损失函数</h3>
<p></p>
]]></description>
</item>
<item>
    <title>YOLO v2深度解读</title>
    <link>https://leeshy-tech.github.io/yolo_v2/</link>
    <pubDate>Sat, 18 Feb 2023 13:14:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/yolo_v2/</guid>
    <description><![CDATA[<h1 id="yolo9000-better-faster-stronger">YOLO9000: Better, Faster, Stronger</h1>
<h2 id="概况">概况</h2>
<p><a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html" target="_blank" rel="noopener noreffer">YOLO9000: Better, Faster, Stronger</a></p>
<p>CVPR 2017</p>
<h2 id="better">Better</h2>
<h3 id="batch-normalization">Batch Normalization</h3>
<p></p>
<h3 id="high-resloution-classifier">High Resloution Classifier</h3>
<p>高分辨率分类器</p>
<p>YOLO v1骨干网络是在224×224图像上训练，再运用到448×448图像的检测。YOLO v2使用448×448图像进行训练。</p>
<p></p>
<p></p>
<h3 id="anchor--dimension-clusters--direct-location-prediction">Anchor / Dimension Clusters / Direct location prediction</h3>
<p>预设一组锚框（anchor boxes），使得不同大小的锚框负责预测不同尺寸的物体。</p>
<p></p>
<p>每个grid cell产生5个anchor，类别信息改为由anchor预测，每个anchor带有4个位置参数，一个置信度参数和20个类别参数。</p>
<p></p>
<p>anchor的5个尺寸由对COCO和Pascal VOC数据集聚类得到。</p>
<p></p>
<p>在anchor坐标回归上，添加sigmoid函数，将BB中心始终约束在grid cell内。</p>
<p></p>
<h3 id="损失函数">损失函数</h3>
<p></p>
<h3 id="fine-grained-features">Fine-Grained Features</h3>
<p>细粒度特征，把浅层网络的特征与深层网络特征叠加，整合不同尺度的特征。</p>
<p></p>
<p></p>
<h3 id="multi-scale-training">Multi-Scale Training</h3>
<p>输入不同尺度的图像，迫使模型学习到不同尺度的特征。输入大尺度图像，模型速度变慢，精度提高。对于小尺度图片，速度变快，精度降低。</p>
<p></p>
<p></p>
<h3 id="所有trick的提升">所有trick的提升</h3>
<p>加入anchor虽然减小了mAP，但是recall大大提升。</p>
<p></p>
<h2 id="faster">Faster</h2>
<p>采用Darknet19作为骨干网络</p>
<p></p>
<p>整体结构：</p>
<p></p>
<h3 id="stronger">Stronger</h3>
<p>检测类别变得更多：将COCO目标检测数据集和Imagenet分类数据集联合训练。</p>
<p></p>
<h2 id="参考">参考</h2>
<p><a href="https://www.bilibili.com/video/BV1Q64y1s74K/?spm_id_from=333.999.0.0&amp;vd_source=c421bf5ed1b524a57c89542f6c02aceb" target="_blank" rel="noopener noreffer">【精读AI论文】YOLO V2目标检测算法</a></p>
]]></description>
</item>
<item>
    <title>YOLO v1深度解读</title>
    <link>https://leeshy-tech.github.io/yolo/</link>
    <pubDate>Fri, 17 Feb 2023 13:04:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/yolo/</guid>
    <description><![CDATA[<h1 id="you-only-look-once-unified-real-time-object-detection">You Only Look Once: Unified, Real-Time Object Detection</h1>
<h2 id="概况">概况</h2>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html" target="_blank" rel="noopener noreffer">You Only Look Once: Unified, Real-Time Object Detection</a></p>
<p>CVPR 2016</p>
<h2 id="训练阶段">训练阶段</h2>
<ul>
<li>将一幅图像分为7×7（S×S）个网格（grid cell），如果某个物体的中心落在某个网格内，就由该网格预测该物体。</li>
<li>每个网格预测 B=2 个bounding box（BB），每个BB回归自身的坐标(x, y, w, h)和置信度confidence共5个值。置信度$ {Pr}(\text { Object }) * \mathrm{IOU}_{\text {pred }}^{\text {truth }}$反映BB含有对象的概率以及预测准确性，为0或IOU。</li>
<li>每个网格预测 C 个类别条件概率${Pr}\left(\text { Class }_{i} \mid \text { Object }\right)$。</li>
<li>输出维度为S×S×（5*B + C），对于PASCAL VOC数据集，输入448×448×3通道的彩色图像，输入7×7×30的张量。</li>
</ul>
<p></p>
<h3 id="网络结构">网络结构</h3>
<p>通过一个深度卷积神经网络抽取图像特征，模型参考GoogleNet</p>
<p></p>
<p>输入是448×448×3通道的彩色图像，经过若干卷积层、池化层变成7×7×1024的feature map，经全连接层回归得到7×7×30的张量。</p>
<h3 id="损失函数">损失函数</h3>
<p></p>
<p></p>
<h2 id="预测阶段">预测阶段</h2>
<p>用模型进行预测，再进行后处理。</p>
<p></p>
<p>模型预测出98个BB，以及每个BB的类别全概率（计算得到）：</p>
<p></p>
<p>它代表BB含有某个类的概率以及BB与对象的匹配程度。</p>
<p>对于特定类别：</p>
<ul>
<li>将小于设定阈值的概率置0，并将所有BB按概率排序，最后进行NMS（Non-maximum suppression，非极大值抑制）</li>
</ul>
<p></p>
<p>非极大值抑制：</p>
<ol>
<li>对于概率最大的BB，计算所有BB与之的IOU，去除IOU大于某设定阈值的BB（概率置0）。（IOU大于某一阈值，就认定两个BB是预测了同一个物体，所以这一步的意义是去除重复的预测框）</li>
<li>对余下的概率最大的BB，重复以上过程。</li>
</ol>
<p></p>
<h2 id="实验结果">实验结果</h2>
<ol>
<li>在同准确率上是最快的，在同速度下是最准的，达到了实时性。</li>
<li>定位错误最多，背景错误比R-CNN少很多，这是因为YOLO输入整张图片，能看到更大范围的图像。</li>
<li>对于集成模型，与R-CNN继承能进一步提升准确率，但是速度没有提升。</li>
<li>泛化性能好，在现实数据集训练的结果在艺术作品上也表现的很好，远超其他方法。</li>
</ol>
<h2 id="参考">参考</h2>
<p><a href="https://www.bilibili.com/video/BV15w411Z7LG?p=4&amp;vd_source=c421bf5ed1b524a57c89542f6c02aceb" target="_blank" rel="noopener noreffer">【精读AI论文】YOLO V1目标检测，看我就够了</a></p>
]]></description>
</item>
<item>
    <title>目标检测评估指标mAP</title>
    <link>https://leeshy-tech.github.io/eval/</link>
    <pubDate>Tue, 07 Feb 2023 21:26:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/eval/</guid>
    <description><![CDATA[<h2 id="目标检测任务">目标检测任务</h2>
<p>目标检测任务是计算机视觉领域的一个分支 ，其关注图片中物体的类别和位置信息，给定一张图片，需要给出图片中感兴趣物体的类别、位置和置信度，位置通过一个矩形的检测框来表示。</p>
<p></p>
<h2 id="特有名词">特有名词</h2>
<p>ground truth box：人工标注的真实框，相当于标准答案。</p>
<p>BB(bounding box)：预测框</p>
<p>pipeline：流程（流水线）</p>
<p>confidence scores：置信度，用于评估一个预测框中含有某个对象的概率。</p>
<h2 id="mapmean-average-precision">mAP（mean Average Precision）</h2>
<h3 id="iou">IoU</h3>
<p>IoU(intersection over union)直译为交并比（交集除以并集）</p>
<p></p>
<p>从图形可以看出，IoU用来衡量两个框（预测框和真实框）之间的相似性。IoU越大，两个框越相似，说明模型预测的越好。</p>
<h3 id="tpfnfptn">TP、FN、FP、TN</h3>
<p>首先考虑以下的混淆矩阵：</p>
<p></p>
<p>与一般的分类问题相比，目标检测任务中True Positives、False Positives、True Negatives和 False Negatives的鉴别会更复杂：</p>
<ul>
<li>
<p>对于模型给出的预测框，我们使用置信度阈值来评估一个框是Positive还是Negative。</p>
</li>
<li>
<p>对于Positive预测框，计算与所有ground truth的IoU，取最大值，通过IoU阈值来评估预测为True Positive还是False Positive。</p>
</li>
<li>
<p>ground truth的数量减去True Positive即为False Negative的数量。</p>
</li>
</ul>
<h3 id="precisionrecall">Precision、Recall</h3>
<p>精度
$$
\text { Precision }=\frac{T P}{T P+F P}
$$
召回率
$$
\text { Recall }=\frac{T P}{T P+F N}
$$
recall和precision是模型性能两个不同维度的度量：</p>
<ul>
<li>
<p>recall度量的是「查全率」，所有的正样本是不是都被检测出来了。比如在肿瘤预测场景中，要求模型有更高的recall，不能放过每一个肿瘤。</p>
</li>
<li>
<p>precision度量的是「查准率」，在所有检测出的正样本中是不是实际都为正样本。比如在垃圾邮件判断等场景中，要求有更高的precision，确保放到回收站的都是垃圾邮件。</p>
</li>
</ul>
<p>这两个参数是矛盾的，二者不可能同时到达最大值，将recall-precision分别做横纵轴绘制图像，图像下面积即为AP（average precision）。</p>
<p>mAP（mean Average Precision）为全类平均正确率，对所有类别的AP取平均值即为mAP。衡量模型在一个数据集上的所有类别的识别性能。</p>
<p></p>
]]></description>
</item>
</channel>
</rss>
