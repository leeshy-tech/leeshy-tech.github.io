<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>News Recommendation - 标签 - Leeshy&#39;s Blog | To be humble</title>
        <link>https://leeshy-tech.github.io/tags/news-recommendation/</link>
        <description>News Recommendation - 标签 - Leeshy&#39;s Blog | To be humble</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>saili@bupt.edu.cn (Leeshy)</managingEditor>
            <webMaster>saili@bupt.edu.cn (Leeshy)</webMaster><lastBuildDate>Tue, 24 May 2022 17:41:06 &#43;0800</lastBuildDate><atom:link href="https://leeshy-tech.github.io/tags/news-recommendation/" rel="self" type="application/rss+xml" /><item>
    <title>论文笔记——AMM: Attentive Multi-field Matching for News Recommendation Matching</title>
    <link>https://leeshy-tech.github.io/ammattentive_multi-field_matching/</link>
    <pubDate>Tue, 24 May 2022 17:41:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/ammattentive_multi-field_matching/</guid>
    <description><![CDATA[<h1 id="amm-attentive-multi-field-matching-for-news-recommendation">AMM: Attentive Multi-field Matching for News Recommendation</h1>
<h2 id="论文概况">论文概况</h2>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3404835.3463232" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/abs/10.1145/3404835.3463232</a></p>
<p>SIGIR &lsquo;21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</p>
<p>July 2021 Pages 1588–1592</p>
<p><a href="https://doi.org/10.1145/3404835.3463232" target="_blank" rel="noopener noreffer">https://doi.org/10.1145/3404835.3463232</a></p>
<p>自翻：<a href="https://github.com/leeshy-tech/PaperTranslate/blob/main/AMMAttentive_Multi-field_Matching.md" target="_blank" rel="noopener noreffer">https://github.com/leeshy-tech/PaperTranslate/blob/main/AMMAttentive_Multi-field_Matching.md</a></p>
<h2 id="引言">引言</h2>
<p>​		用户兴趣与候选新闻的准确匹配是准确的个性化新闻推荐的前提。现有的方法主要是通过顺序或注意力模型聚合用户之前浏览过的新闻来学习用户的兴趣向量，然后将其与候选新闻向量进行匹配，取得了相当大的进展。例如，NPA [12] 根据候选新闻和之前点击的新闻之间的相似性来学习用户表示。 LSTUR [1] 使用 GRU 网络从点击的新闻中模拟短期和长期的用户兴趣。 NAML [11] 和 NRMS [13] 应用注意力网络从点击的新闻中学习用户表示。 FIM [9] 为每个新闻提取多级表示，并通过卷积执行细粒度匹配。然而，这些方法中的大多数将每个用户和新闻表示为单个向量，这可能会丢失文本匹配信号（例如，词级关系）。因此，我们的方法不是简单地将用户浏览的新闻建模为一个整体，而是关注每个浏览的候选新闻对之间的关系，以捕获细粒度的语义匹配表示。此外，这种对设计是在线服务友好的，匹配的表示可以离线存储，以避免重复计算。在这里，我们首先使用 BERT 来学习每个浏览候选新闻对在不同语义级别的匹配表示，然后将它们聚合到最终的用户新闻匹配信号。</p>
<p>​	此外，上述方法基于单视图新闻信息[1,9,13]或多视图信息[11]学习新闻之间的语义相关性，它们仅利用字段内语义相关性（例如，标题-标题，正文-正文）。 但有时，跨领域信息（例如标题-摘要、标题-正文）可能有利于新闻匹配。 比如新闻𝑎的标题信息丰富，而新闻𝑏的标题很吸引人，新闻𝑎的标题和新闻𝑏的正文匹配可能更好。 因此，与每条新闻相关的多个字段可能包含互补信息，这些信息促使我们学习多字段（字段内和跨字段）匹配表示。</p>
<p>​	在本文中，我们提出了一种注意力多字段匹配（AMM）框架，该框架捕获每个浏览候选新闻对的多字段语义匹配表示，然后将这些表示聚合为最终的用户新闻匹配信号，用于新闻推荐。 这项工作的主要贡献总结如下：</p>
<ul>
<li>我们提出了一种新的方法，专注于分离的浏览候选新闻对的匹配，以捕获文本语义匹配信号，这对在线服务很友好。</li>
<li>我们设计了 AMM 框架，以在领域内和跨领域的方式中提取多领域匹配表示，以深入探索用户的兴趣。</li>
<li>我们对两个公共基准数据集进行了实验，以证明我们方法的有效性。</li>
</ul>
<h2 id="我们的方法">我们的方法</h2>
<p>图 1 显示了 AMM 的整体架构。 首先，我们为每个浏览的候选新闻构建多字段对输入。 然后，匹配编码器用于提取每对输入的匹配表示。 最后，我们将所有对的匹配表示聚合到用户新闻信号以估计点击概率。</p>
<p></p>
<h3 id="21-多头部匹配">2.1 多头部匹配</h3>
<p>​	不同的新闻字段通常可以相互补充。 为了更好地编码浏览-候选匹配，我们利用字段内和跨字段匹配作为多字段信息来增强匹配表示。</p>
<p>​	给定用户浏览的新闻$\left[N_{1}, N_{2}, \ldots, N_{i}\right]$和候选新闻$N_x$，对于每个浏览-候选对$\left(N_{i}, N_{c}\right)$，我们建立多头部对输入$\left[N_{i}^{m}, N_{c}^{n}\right]$，其中$m \in{t, a, b},n \in{t, a, b}$，t、a、b分别是新闻的标题、摘要、主体。我们将新闻标题$N^t$表示为词向量$\left[w_{1}^{t}, w_{2}^{t}, \ldots, w_{\left|N^{t}\right|}^{t}\right]$，新闻摘要$N^a$表示为$\left[w_{1}^{a}, w_{2}^{a}, \ldots, w_{|N a|}^{a}\right]$，新闻主体$N^b$表示为$\left[w_{1}^{b}, w_{2}^{b}, \ldots, w_{\left|N^{\prime}\right|}^{b}\right]$，其中$\left|N^{t}\right|\left|N^{a}\right|$和$\left|N^{b}\right|$分别表示标题、摘要、主体的长度。</p>
<p>​	在这项研究中，如图 1 所示，不仅像$\left[N_{i}^{t}, N_{c}^{t}\right]$（标题和标题）这样的字段内对输入，而且像$\left[N_{i}^{t}, N_{c}^{b}\right]$（标题和正文）这样的跨字段对输入都被获得 ，这些输入对中的每一个都被馈送到匹配编码器以获得其匹配表示。 我们最终将这些表示串接为浏览候选新闻对的多字段匹配表示。</p>
<h3 id="22-匹配编码器">2.2 匹配编码器</h3>
<p>该模块用于学习每个浏览过的新闻和候选新闻之间的语义匹配。 我们将来自多字段匹配的浏览-候选多字段对作为输入，然后将其输入到来自 Transformers (BERT) [4] 的双向编码器表示中，以进行匹配表示学习。</p>
<ul>
<li>
<p>多头部自注意层</p>
<p>该子层旨在捕获句子的上下文感知语义匹配。 使用多头，来自不同位置的不同语义子空间的信息是可联合学习的，这对于捕获不同标记之间的匹配信号非常有帮助。</p>
</li>
<li>
<p>位置智慧型前馈网络</p>
<p>这个子层的目的是赋予模型非线性和不同维度之间的相互作用，这是一个完全连接的前馈网络(FFN)，它由两个线性变换组成，中间有一个ReLU激活。</p>
</li>
</ul>
<p>另外，对每个子层进行残差连接和层归一化处理。最后，我们可以通过该匹配编码器得到每个浏览-候选新闻的多字段对的匹配表示。</p>
<h3 id="23-匹配聚合器">2.3 匹配聚合器</h3>
<p>​	匹配聚合器用于聚合每个浏览新闻和候选新闻的匹配表示，以获得最终的用户新闻匹配信号。我们使用以下三种类型的聚合器获得最终的用户新闻匹配表示$\tilde{M}$：</p>
<ul>
<li>
<p>最大/平均聚合器：对的匹配嵌入直接取最大或平均作为用户-新闻匹配嵌入。
$$
\tilde{M}=\text { Aggregate }(M)=\max / \operatorname{mean}(M)\tag 5
$$</p>
</li>
<li>
<p>注意力聚合器：使用门限注意网络 [2] 来学习 𝑀 的组合权重，它应用具有$W_h$和$b_h$的全连接神经网络，使用 tanh 作为第一层激活函数，然后使用$W_o$和$b_o$构造第二个全连接神经网络来学习在等式（6）中定义的组合权重 𝑟 ，最后在等式（7）中计算词向量的线性组合。
$$
\tilde{M}=\text { Aggregate }(M)=\mathbf{r}^{\top} M\tag 6
$$</p>
<p>$$
r=\operatorname{softmax}\left(\tanh \left(\tilde{E} W_{h}+b_{h}\right) W_{o}+b_{o}\right)\tag 7
$$</p>
</li>
</ul>
<h3 id="24-点击预测">2.4 点击预测</h3>
<p>最后，我们介绍点击预测模块。 将<strong>用户-新闻匹配表示</strong>表示为$\tilde{M}$，点击概率 𝑦 通过应用全连接层计算：
$$
y=\operatorname{softmax}\left(\tilde{M} W^{o}+b^{o}\right)\tag 8
$$
其中$W^{o} \in \mathbb{R}^{d^{\prime} \times 1}$和$b^{o} \in \mathbb{R}^{1}$是可学习参数。</p>
<h2 id="3-实验">3 实验</h2>
<h3 id="31-实验设置">3.1 实验设置</h3>
<h4 id="数据集">数据集</h4>
<ul>
<li>MIND</li>
<li>Adressa</li>
</ul>
<h4 id="评价指标">评价指标</h4>
<p>​	我们使用 AUC、MRR、nDCG@𝐾，其中𝐾 = 5, 10 用于 MIND，𝐾 = 1, 3 用于 Adressa，作为我们的评估指标。 性能是所有展示日志中这些指标的平均值。</p>
<h3 id="32-性能评估">3.2 性能评估</h3>
<p>​	我们通过将 AMM 与几种基准方法进行比较来评估 AMM 的性能，</p>
<ul>
<li>LibFM [8]，分解机（FM）；</li>
<li>DeepFM[6]：一种结合了FM和神经网络的深度分解机；</li>
<li>DKN[10]，一种基于知识感知CNN的深度新闻推荐方法；</li>
<li>NPA[12]，引入注意力机制来选择重要的词和新闻；</li>
<li>NAML [11]，一种具有注意力多视图学习的神经新闻推荐方法；</li>
<li>LSTUR [1]，它使用 GRU 从点击历史中对短期和长期兴趣进行建模；</li>
<li>NRMS [13]，它使用多头自注意力来学习用户和新闻表示；</li>
<li>FIM[9]，一种用于神经新闻推荐的细粒度兴趣匹配方法；</li>
<li>AMM，我们的方法。</li>
</ul>
<p></p>
<ul>
<li>使用深度神经网络提取新闻语义表示的方法（3-8）比基于特征的方法（1-2）表现更好。这种性能改进应归功于更好的新闻表示方法。</li>
<li>在方法（3-8）中，NAML 在 MIND 中表现最好，NRMS 在 Adressa 中表现最好，这是因为 NAML 使用了不同种类的新闻信息，而 NRMS 应用了多头自注意力来进一步捕捉单词和点击新闻互动。 FIM 在所有基线中的 Adressa 中表现最好，因为它可以捕获更细粒度的兴趣匹配信号。</li>
<li>我们提出的 AMM 通过考虑分离的浏览候选新闻对和跨字段匹配来捕获多字段的更好的文本语义匹配，在所有指标方面在两个数据集上实现了最佳性能。</li>
</ul>
<h3 id="33-消融实验">3.3 消融实验</h3>
<p></p>
<ul>
<li>比较了AMM与三个单一字段(标题、摘要、正文)在MIND上的性能。
<ul>
<li>带有标题和正文的AMM的表现优于带有摘要的AMM，这表明标题和正文比摘要更有利于新闻表征。</li>
<li>此外，单个标题或正文的AMM的性能优于最佳基准，表明了分离策略引入文本语义匹配的必要性。</li>
</ul>
</li>
<li>通过实验验证了多领域匹配的有效性。
<ul>
<li>AMM多字段比AMM内字段性能更好，这可能是因为通过跨字段匹配可以融合更多的互补信息，字段内和多字段信息的集成可以提高用户新闻匹配，捕捉细粒度用户的兴趣。</li>
</ul>
</li>
<li>探讨了不同匹配聚合器的有效性。
<ul>
<li>我们可以看到带有注意力聚合器的 AMM 的性能略好于带有最大+平均池化的 AMM。 而且，最大+平均池化的表现也比 最佳基准方法好很多，验证了多字段语义匹配的有效性，这些简单的操作让在线服务更有可能。</li>
</ul>
</li>
</ul>
<h2 id="结论">结论</h2>
<p>​	在本文中，我们提出了一种新颖的注意力多领域匹配（AMM）新闻推荐框架。 我们建议以新闻分离的方式学习用户-新闻匹配表示，捕获每对浏览候选新闻的匹配表示，然后将这些表示聚合为最终匹配信号，从而获得细粒度的语义匹配信息以及它是在线服务友好的。 此外，我们的方法同时考虑了字段内和跨字段作为多字段匹配，利用了字段间的互补信息，提高了新闻对的匹配表示。 两个公共基准数据集的实验结果证明了 AMM 的最新性能。</p>
]]></description>
</item>
<item>
    <title>论文笔记——Personalized News Recommendation with Knowledge-aware Interactive Matching</title>
    <link>https://leeshy-tech.github.io/personalized_news_recommendation/</link>
    <pubDate>Mon, 23 May 2022 15:03:06 &#43;0800</pubDate><author>saili@bupt.edu.cn (Leeshy)</author><guid>https://leeshy-tech.github.io/personalized_news_recommendation/</guid>
    <description><![CDATA[<h1 id="personalized-news-recommendation-with-knowledge-aware-interactive-matching">Personalized News Recommendation with Knowledge-aware Interactive Matching</h1>
<p>基于知识感知交互匹配的个性化新闻推荐</p>
<h2 id="论文概况">论文概况</h2>
<p><a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462861" target="_blank" rel="noopener noreffer">https://dl.acm.org/doi/abs/10.1145/3404835.3462861</a></p>
<p>SIGIR &lsquo;21: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</p>
<p>July 2021 Pages 61–70</p>
<p><a href="https://doi.org/10.1145/3404835.3462861" target="_blank" rel="noopener noreffer">https://doi.org/10.1145/3404835.3462861</a></p>
<p>自翻：<a href="https://github.com/leeshy-tech/PaperTranslate/blob/main/Personalized_news_recommendation_with_%20Knowledge-aware_Interactive_Matching.md" target="_blank" rel="noopener noreffer">https://github.com/leeshy-tech/PaperTranslate/blob/main/Personalized_news_recommendation.md</a></p>
<h2 id="摘要">摘要</h2>
<p>​	在本文中，我们提出了一种用于新闻推荐的知识感知交互式匹配方法。我们的方法以交互方式对候选新闻和用户兴趣进行建模，以促进它们的准确匹配。我们设计了一个知识感知新闻协同编码器，在知识图谱的帮助下捕获它们在语义和实体中的相关性，交互式地学习点击新闻和候选新闻的表示。我们还设计了一个用户新闻协同编码器来学习候选新闻感知用户兴趣表示和用户感知候选新闻表示，以实现更好的兴趣匹配。在两个真实世界数据集上的实验验证了，我们的方法可以有效地提高新闻推荐的性能。</p>
<h2 id="1-引言">1 引言</h2>
<p>​	现有方法通常根据其文本信息对候选新闻进行建模，并以独立的方式从用户的点击历史中推断出用户兴趣[21, 37]。 候选新闻文章可能包含多个方面和实体 [18、33]，并且用户可能有多个兴趣 [32]。 因此，候选新闻和用户兴趣的独立建模可能不如兴趣匹配[31]。</p>
<p>​	在本文中，我们探索更好地模拟候选新闻和用户兴趣之间的相关性，以实现准确的兴趣匹配。 我们的论文受到以下观察的启发。</p>
<p></p>
<ol>
<li>
<p>候选新闻可能涵盖不同的方面和实体，并且用户可能有多种兴趣。</p>
<p>例如，图中 候选新闻2 与篮球明星和政治家有关，含有多个实体（库里和特朗普）。示例用户对车、音乐、体育等领域感兴趣，第二个候选新闻只能匹配特定的用户兴趣即“体育”，但是用户可能只对实体“库里”感兴趣。所以如果独立建模，用候选新闻匹配用户兴趣的效果是较差的。</p>
</li>
<li>
<p>候选新闻和点击新闻的语义匹配有助于更准确地进行兴趣匹配。</p>
<p>例如，第二条点击新闻也与第一条候选新闻有语义相关性，因为它们提到了相同的事件。基于这些语义相关性，我们可以推断用户可能对第一个候选新闻感兴趣。</p>
</li>
<li>
<p>借助知识图谱，点击新闻和候选新闻中实体之间的知识匹配也有助于了解用户对候选新闻的兴趣。</p>
<p>例如，第 4 条点击新闻中的实体“史蒂夫·科尔”与第 2 条候选新闻中的实体“斯蒂芬·库里”具有内在关联，因为前者和后者分别是“NBA”勇士队的球员和教练。根据知识匹配，我们可以推断出用户可能对第二个候选新闻感兴趣。因此，在语义和知识层面利用点击新闻和候选新闻之间的相关性有利于兴趣匹配。</p>
</li>
</ol>
<p>​	在本文中，我们提出了一种用于个性化新闻推荐的知识感知交互式匹配框架（命名为KIM）。我们的方法可以交互地对候选新闻和用户兴趣进行建模，以学习候选新闻感知的用户兴趣表示和用户感知的候选新闻表示，从而更准确地匹配用户兴趣和候选新闻。在该框架中，我们提出了一种知识协同编码器，借助知识图谱从点击新闻中的实体与候选新闻中的实体之间的相关性来建模用户对候选新闻的兴趣。</p>
<p>​	更具体地说，我们</p>
<ol>
<li>
<p>首先提出了一个<strong>图协同注意网络（ graph co-attention network）</strong>：</p>
<p>它通过选择和聚合对兴趣匹配有用的邻居，从知识图谱中学习实体的表示。</p>
</li>
<li>
<p>进一步提出了使用<strong>实体协同注意力网络（ entity co-attention network）</strong>：</p>
<p>它通过捕获实体之间的相关性来交互式地学习点击新闻和候选新闻的基于知识的表示。</p>
</li>
<li>
<p>提出了一种<strong>语义协同编码器（ semantic coencoder）</strong>：</p>
<p>通过对文本之间的语义相关性进行建模，交互式地学习用户点击新闻和候选新闻的基于语义的表示。新闻的统一表示被表述为基于知识和语义的表示的聚合。</p>
</li>
<li>
<p>进一步提出了一种<strong>用户新闻联合编码器（ user-news co-encoder）</strong>：</p>
<p>用于从点击新闻和候选新闻的表示中构建候选<em>新闻感知的用户兴趣表示</em>和<em>用户感知的候选新闻表示</em>，以更好地模拟用户对候选新闻的兴趣。最后，根据候选新闻的表示与用户兴趣之间的相关性对候选新闻进行排名。</p>
</li>
</ol>
<p>我们对两个真实的数据集进行了广泛的实验，并表明我们的方法可以有效地提高新闻推荐的性能并优于其他基准方法。</p>
<h2 id="2-相关的工作">2 相关的工作</h2>
<p>​	现有方法通常通过内容对候选新闻进行建模，并根据点击新闻独立建模用户兴趣，然后根据候选新闻和用户兴趣的相关性进行匹配。 只有一部分候选新方面和用户兴趣对匹配用户兴趣和候选新闻有用。然而，这些方法独立地对候选新闻和用户兴趣进行建模，这对于进一步的兴趣匹配可能较差。与这些方法不同的是，在 KIM 方法中，我们提出了一个知识感知的交互式匹配框架，在考虑相关性的情况下对候选新闻和用户兴趣进行交互建模，可以更好地将用户兴趣与候选新闻进行匹配。</p>
<p>​	一些方法以候选感知的方式模拟用户兴趣。事实上，候选新闻可能包含多个方面和实体，其中只有一部分可能与用户兴趣相匹配。然而，这些方法在没有考虑目标用户的情况下对候选新闻进行建模，这对于进一步将用户兴趣与候选新闻进行匹配可能较差。与这些方法不同，我们的 KIM 方法在考虑目标用户的情况下对候选新闻进行建模。此外，这些方法在没有考虑相关性的情况下对点击新闻和候选新闻进行建模，这对于进一步衡量候选新闻和从点击新闻推断出的用户兴趣之间的相关性也可能不是最优的。与这些方法不同，KIM 可以交互地学习点击新闻和候选新闻的表示，以实现更好的兴趣匹配。</p>
<h2 id="3-方法论">3 方法论</h2>
<h3 id="问题表述">问题表述</h3>
<p>​	给定一个用户𝑢和一个候选新闻$n^c$，我们需要计算相关性分数𝑧来衡量用户𝑢对候选新闻内容$n^c$的兴趣。 然后根据相关性得分对不同的候选新闻进行排名并推荐给用户𝑢。 用户𝑢与他/她点击的新闻集相关联。 每个新闻 𝑛 都与其文本 𝑇 和文本中的实体 𝐸 相关联。 此外，还有一个知识图 G 用于提供实体之间的相关性。 它包含实体和实体之间的关系。 G 中的每个实体 𝑒 与其嵌入相关联，e 基于知识图进行预训练。</p>
<p></p>
<h3 id="kim-knowledge-aware-interactive-matching的框架">KIM（ Knowledge-aware Interactive Matching）的框架</h3>
<p>​	KIM 包含两个主要模块。：</p>
<ol>
<li>
<p>第一个是知识感知新闻协同编码器（knowledge-aware news co-encoder），它通过捕获语义和知识层面的相关性，交互式地学习用户<strong>点击新闻</strong>和<strong>候选新闻</strong>的<strong>知识感知表示</strong>（ knowledge-aware representations）。</p>
</li>
<li>
<p>第二个是用户新闻协同编码器（user-news co-encoder），它从上个模块生成的用户<strong>点击新闻的表征</strong>和<strong>候选新闻表征</strong>中交互学习候选新闻感知的用户兴趣表征（ candidate news-aware user interest representation）u和用户感知的候选新闻表征（ user-aware candidate news representation）。</p>
</li>
</ol>
<h3 id="知识感知新闻协同编码器knowledge-aware-news-co-encoder">知识感知新闻协同编码器（knowledge-aware news co-encoder）</h3>
<p>​	它从文本和文本中的实体中交互式地学习用户点击的新闻$n_u$和候选新闻$n_c$的表示，它包含三个子模块。</p>
<p></p>
<ol>
<li>
<p>第一个是知识协同编码器（Knowledge co-encoder）</p>
<p>对于点击新闻$n_u$和候选新闻$n_c$，它基于知识图谱从<strong>实体</strong>之间的相关性中交互式地学习基于知识的表示${k}^{u}$$和 $${k}^{c}$。</p>
<p>它包含三个组件：</p>
<ul>
<li>图注意网络（ graph attention network）</li>
<li>图协同注意网络（ graph co-attention network）</li>
<li>实体协同注意网络（ entity co-attention network）</li>
</ul>
<p></p>
</li>
<li>
<p>第二个是语义协同编码器（semantic co-encoder）</p>
<p>对于点击新闻$n_u$和候选新闻$n_c$，它交互式地学习基于语义的表示${t}^{u}$和${t}^{c}$，以根据<strong>文本</strong>之间的语义相关性来模拟用户对候选新闻的兴趣。</p>
<p></p>
</li>
<li>
<p>最后，对于点击新闻$n_u$或候选新闻$n_c$，我们投影其基于<strong>知识</strong>和<strong>语义</strong>的表示来学习统一的新闻表示${n}^{u}$或${n}^{c}$。</p>
</li>
</ol>
<h3 id="用户-新闻-协同编码器">用户-新闻 协同编码器</h3>
<p>​	它从用户点击的新闻和候选新闻的表示中学习候选新闻感知的用户兴趣表示和用户感知的候选新闻表示。</p>
<p>​	通常，用户的兴趣是多样的，只有一部分可以与候选新闻匹配[20]。因此，学习候选<strong>新闻感知的</strong>用户兴趣表示可以更好地建模用户兴趣以匹配候选新闻。</p>
<p>​	类似地，候选新闻可能涵盖多个方面，用户可能只对其中的一部分感兴趣 [33, 34]。因此，学习<strong>用户感知的</strong>候选新闻表示也有利于兴趣匹配。</p>
<p>​	因此，我们应用新闻协同注意网络来学习候选新闻感知用户表示和用户感知候选新闻表示。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集">数据集</h3>
<ul>
<li>MIND2</li>
<li>Feeds</li>
</ul>
<p>此外，我们在实验中使用了 WikiData 作为知识图谱。</p>
<h3 id="性能评估">性能评估</h3>
<p>​	我们将 KIM 与几种最先进的个性化新闻推荐方法进行比较，如下所示：</p>
<ul>
<li>EBNR [21]：通过 GRU 网络从用户的点击历史中表示用户兴趣。</li>
<li>DKN [32]：将多通道 CNN 网络 [15] 应用于新闻标题中对齐的单词和实体的嵌入，以学习新闻表示。</li>
<li>DAN [47]：通过 CNN 网络从新闻标题的单词和实体中学习新闻表示，并通过细心的 LSTM 网络 [8] 学习用户兴趣表示。</li>
<li>NAML [33]：通过多个注意力集中的 CNN 网络从新闻标题、正文、类别和子类别中学习新闻表示。</li>
<li>NPA [34]：使用具有个性化注意查询的注意网络来学习新闻和用户表示。</li>
<li>LSTUR [1]：通过 GRU 网络从用户最近点击的新闻中建模短期用户兴趣，并通过用户 ID 嵌入建模长期用户兴趣。</li>
<li>NRMS [37]：通过多头自注意力网络对新闻内容和用户点击行为进行建模。</li>
<li>FIM [31]：通过CNN网络从用户点击新闻和候选新闻的文本中匹配用户和新闻。</li>
<li>KRED [18]：通过图注意力网络从新闻中的实体及其在知识图中的邻居中学习新闻的表示。</li>
</ul>
<p></p>
<p>​	我们重复了五次不同的实验，并在表 2 中列出了不同方法的平均性能和相应的标准差。</p>
<ul>
<li>
<p>KIM 明显优于其他基准方法。</p>
<p>LSTUR、NRMS和KRED这些方法独立地对候选新闻和用户兴趣进行建模，而不考虑它们的相关性。这是因为用户可能对多个领域感兴趣，并且候选新闻也可能包含多个方面和实体。因此，这些方法很难准确匹配用户兴趣和候选新闻，因为它们是在这些方法中独立建模的。</p>
</li>
<li>
<p>KIM 还优于通过考虑候选新闻来建模用户兴趣的基线方法。</p>
<p>例如 DKN、DAN。这是因为候选新闻可能涵盖多个方面，而用户可能只对其中的一部分感兴趣[33、34]。然而，这些方法在没有考虑目标用户的情况下对候选新闻进行建模，这对于进一步将候选新闻与用户兴趣匹配可能较差。</p>
</li>
</ul>
<h3 id="消融实验">消融实验</h3>
<p>​	进行了两项消融研究来评估 KIM 的有效性。首先评估不同信息（即文本和知识）对新闻内容建模的有效性。</p>
<p></p>
<ul>
<li>首先，删除语义信息（即新闻文本）会严重损害 KIM 的性能。这是因为文本通常包含有关新闻内容的丰富信息，对于新闻内容的理解至关重要 [45]。去除语义信息会使新闻表示失去很多重要信息，并且不能准确地对新闻内容进行建模。</li>
<li>其次，在新闻内容建模中去除知识图谱（即知识知识图谱中的实体及其邻居）也会使 KIM 的性能显著下降。这是因为文本信息通常不足以理解新闻内容 [18, 32]。幸运的是，知识图谱包含不同实体之间的丰富关联。此外，用户点击新闻中的实体与候选新闻之间的相关性可以提供语义信息之外的丰富信息，以了解用户对候选新闻的兴趣。因此，将实体信息纳入个性化新闻推荐有可能提高推荐的准确性。</li>
</ul>
<p>​	接下来，我们通过分别用注意力网络替换它们来评估 KIM 中几个重要的注意力网络的有效性。</p>
<ul>
<li>首先，在用户新闻协同编码器中去除<strong>新闻关注网络</strong>后，KIM 的性能变得更差。这是因为用户兴趣可能是多样的，并且只有一部分用户点击的新闻对于建模用户兴趣和候选新闻之间的相关性是有用的[32]。此外，候选新闻内容可能包含多个方面，用户可能只对其中的一部分感兴趣。因此，通过新闻共同关注网络学习候选新闻感知用户兴趣和用户感知候选新闻表示可以更好地捕捉用户对候选新闻的兴趣。</li>
<li>其次，去除<strong>语义协同注意网络</strong>也会损害 KIM 的性能。这是因为点击新闻和候选新闻之间的语义相关性可以帮助理解用户对候选新闻的兴趣。此外，一条候选新闻或一条点击新闻通常包含多个方面，其中只有一部分对兴趣匹配有用。如果它们的语义信息是独立建模的，则很难在语义级别有效地捕捉点击新闻和候选新闻的相关性。因此，通过语义共同注意网络交互式学习点击新闻和候选新闻的基于语义的表示可以更好地捕捉它们之间的相关性，以便将用户兴趣与候选新闻匹配。</li>
<li>同时去除<strong>图协同注意力网络</strong>和<strong>实体协同注意力网络</strong>会导致 KIM 的性能下降。这是因为实体级别的点击新闻和候选新闻之间的相关性对于兴趣匹配也非常有用。此外，如果该方法独立地表示来自其实体的点击新闻和候选新闻，则对于兴趣匹配也是次优的。在KIM方法中，图协同注意力网络和实体协同注意力网络都用于以交互方式捕捉点击新闻和候选新闻实体之间的相关性，可以将丰富的信息融入KIM模型进行兴趣匹配。</li>
</ul>
<h2 id="结论">结论</h2>
<p>​	在本文中，我们提出了一种用于个性化新闻推荐的知识感知交互式匹配框架（命名为 KIM）。该框架旨在对候选新闻和用户兴趣进行交互建模，以实现更准确的兴趣匹配。更具体地说，我们首先提出了一个图协同注意网络，通过选择和聚合它们的邻居的信息来基于知识图对实体进行建模，这些信息为兴趣匹配提供了丰富的信息。我们还提出使用实体协同注意网络从实体之间的相关性中交互式地对点击新闻和候选新闻进行建模。此外，我们建议使用语义共同注意网络从文本之间的语义相关性中交互式地对点击新闻和候选新闻进行建模。此外，我们提出了一种用户新闻协同编码器来学习候选新闻感知用户表示和用户感知候选新闻表示，以更好地捕捉用户兴趣和候选新闻之间的相关性。我们对两个真实世界的数据集进行了广泛的实验。实验结果表明，我们的 KIM 方法显著优于其他基准方法。</p>
]]></description>
</item>
</channel>
</rss>
